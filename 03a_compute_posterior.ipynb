{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ddbf456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import timeit\n",
    "import math\n",
    "import time\n",
    "import sparse\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask.diagnostics import ProgressBar\n",
    "from glob import glob\n",
    "from os import path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ll_Balltree import *\n",
    "%run -i 'll_Balltree.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d86b892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-97.98001098632812,\n",
       " -76.45999145507812,\n",
       " 18.140000343322754,\n",
       " 31.899998664855957,\n",
       " 345,\n",
       " 539,\n",
       " 0.03997802734375,\n",
       " 0.03999900817871094)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputDir = 'data/posterior_computation_data/'\n",
    "gom_masks = xr.open_dataset(outputDir + 'gom_masks.nc')\n",
    "\n",
    "# GLOBAL CONSTANTS\n",
    "MIN_LON = np.min(gom_masks['lon'].values)\n",
    "MAX_LON = np.max(gom_masks['lon'].values)\n",
    "MIN_LAT = np.min(gom_masks['lat'].values)\n",
    "MAX_LAT = np.max(gom_masks['lat'].values)\n",
    "\n",
    "#domain width and height (cell counts)\n",
    "LAT_SIZE = gom_masks.dims['lat']\n",
    "LON_SIZE = gom_masks.dims['lon']\n",
    "\n",
    "#cell size\n",
    "D_LON = gom_masks[\"lon\"][1].values - gom_masks[\"lon\"][0].values\n",
    "D_LAT = gom_masks[\"lat\"][1].values - gom_masks[\"lat\"][0].values\n",
    "\n",
    "BIN_CELL_LATS = gom_masks.bin_cell_lats.values\n",
    "BIN_CELL_LONS = gom_masks.bin_cell_lons.values\n",
    "\n",
    "MIN_LON, MAX_LON, MIN_LAT, MAX_LAT,LAT_SIZE,LON_SIZE, D_LON,D_LAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9927d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load domain_cell_tree\n",
    "fileObj = open(outputDir + 'output_dict.obj', 'rb')\n",
    "output = pickle.load(fileObj)\n",
    "fileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3332267f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188, 114024, 30, 36)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cell_beaching = output['n_cell_beaching']\n",
    "n_cell_source = output['n_cell_source']\n",
    "n_window_beaching = output['n_window_beaching'] \n",
    "n_window_source = output['n_window_source']\n",
    "\n",
    "particle_count = output['particle_count']\n",
    "\n",
    "beaching_cells = output['beaching_cells'] \n",
    "beaching_cell_tree = output['beaching_cell_tree']\n",
    "\n",
    "source_cell_mask = output['source_cell_mask']\n",
    "source_cells = output['source_cells']\n",
    "source_cell_tree = output['source_cell_tree']\n",
    "\n",
    "beaching_windows = output['beaching_windows']\n",
    "source_windows = output['source_windows']\n",
    "d = output['d']\n",
    "beaching_ym_mat = output['beaching_ym_mat']\n",
    "source_ym_mat = output['source_ym_mat']\n",
    "n_cell_beaching,n_cell_source,n_window_beaching, n_window_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cf843d",
   "metadata": {},
   "source": [
    "Compute Likelihood Quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e972e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(outputDir + 'trajectory_mat_st.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ddd1532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Likelihood Matrix: 100%|██████████| 36/36 [00:38<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized Likelihood_mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_170537/4090729719.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  Likelihood_mat = np.nan_to_num(Likelihood_mat / np.sum(Likelihood_mat, axis = 1)[:,None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   done in 126.62137913703918\n"
     ]
    }
   ],
   "source": [
    "Likelihood_mat = np.zeros(((n_cell_source*n_window_source),n_cell_beaching*(d+1)))\n",
    "\n",
    "for source_time in tqdm(range(n_window_source), desc=\"Calculating Likelihood Matrix\"):\n",
    "    # Create Output Matrix row block\n",
    "    out = np.zeros((n_cell_source, n_cell_beaching * (d+1)))\n",
    "\n",
    "    # Find all observations with indexed source time\n",
    "    traj_idx, obs_idx = np.where(ds.source_traj_times == source_time)\n",
    "\n",
    "    # Collect the source cells, beaching windows, beaching cells\n",
    "    df ={\"ll_row\": ds.source_traj_locs.values[traj_idx, obs_idx],\"beaching_window\": ds.beaching_times.values[traj_idx], \"beaching_cell\": ds.beaching_locs.values[traj_idx]}\n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    # Find frequency counts for each (source cells, beaching windows, beaching cells) combination \n",
    "    freqs = df.value_counts().reset_index().astype(int)\n",
    "\n",
    "    # Shift beaching window to one of the (d+1) possible beaching windows from the indexed source time\n",
    "    freqs['beaching_window'] = source_time - freqs['beaching_window'] \n",
    "\n",
    "    # Find the corresponding likelihood column index\n",
    "    freqs['ll_col'] = (freqs['beaching_window'] * n_cell_beaching) + freqs['beaching_cell']\n",
    "\n",
    "    # Assign counts at corresponding indicies\n",
    "    out[freqs['ll_row'].values,freqs['ll_col'].values] = freqs['count'].values\n",
    "\n",
    "    # Assign to likelihood matrix\n",
    "    Likelihood_mat[((source_time)*n_cell_source):((source_time+1)*n_cell_source),:] = out\n",
    "\n",
    "# Row Probabilities\n",
    "print('normalized Likelihood_mat')\n",
    "tic=time.time()\n",
    "Likelihood_mat = np.nan_to_num(Likelihood_mat / np.sum(Likelihood_mat, axis = 1)[:,None])\n",
    "print('   done in',time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee13e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood Matrix Density - % 0.12753479626145292\n"
     ]
    }
   ],
   "source": [
    "num_nonzero_ll_counts = np.shape((np.where(Likelihood_mat >0.0)))[1]\n",
    "num_all_ll_counts = n_cell_source*n_window_source*n_cell_beaching*(d+1)\n",
    "ll_mat_density = 100 * num_nonzero_ll_counts / num_all_ll_counts\n",
    "\n",
    "print(\"Likelihood Matrix Density - %\", ll_mat_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ced47912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Likelihood_mat_rowSums = np.sum(Likelihood_mat, axis = 1)\n",
    "np.unique(Likelihood_mat_rowSums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4308f11",
   "metadata": {},
   "source": [
    "Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c901470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = xr.open_dataset(outputDir + 'prior.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "def9890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_prior_source_cells = prior.l_prior.values[:, source_cell_mask[0], source_cell_mask[1]]\n",
    "r_prior_source_cells = prior.r_prior.values[:, source_cell_mask[0], source_cell_mask[1]]\n",
    "f_prior_source_cells = prior.f_prior.values[:, source_cell_mask[0], source_cell_mask[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b18417a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 114024)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source Windows X Source Cells\n",
    "np.shape(l_prior_source_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e34d3862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unnormalized posteriors\n",
      "   done in 86.83976912498474\n"
     ]
    }
   ],
   "source": [
    "# Compute unnormalized posteriors\n",
    "print('unnormalized posteriors')\n",
    "tic=time.time()\n",
    "l_nn_post = Likelihood_mat * np.ravel(l_prior_source_cells)[:, None] \n",
    "r_nn_post = Likelihood_mat * np.ravel(r_prior_source_cells)[:, None] \n",
    "f_nn_post = Likelihood_mat * np.ravel(f_prior_source_cells)[:, None]\n",
    "print('   done in',time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f16dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset(\n",
    "    {\n",
    "        'l_nn_post': (['source_window_cells', 'dbeaching_windows_cells'], l_nn_post),\n",
    "        'r_nn_post': (['source_window_cells', 'dbeaching_windows_cells'], r_nn_post),\n",
    "        'f_nn_post': (['source_window_cells', 'dbeaching_windows_cells'], f_nn_post),\n",
    "    },\n",
    "    coords={\n",
    "        'source_window_cells': np.arange(n_cell_source*n_window_source),\n",
    "        'dbeaching_windows_cells': np.arange(n_cell_beaching*(d+1)),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e05e2b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re-chunking\n",
      "   done in 138.3991813659668\n"
     ]
    }
   ],
   "source": [
    "chunksize = {'source_window_cells': n_cell_source,\n",
    "        'dbeaching_windows_cells': n_cell_beaching}\n",
    "\n",
    "print('re-chunking')\n",
    "tic=time.time()\n",
    "ds = ds.chunk(chunksize)\n",
    "print('   done in',time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fafec500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 183.35 s\n"
     ]
    }
   ],
   "source": [
    "delayedObj = ds.to_zarr(outputDir + 'nn_post.zarr',compute=False)\n",
    "with ProgressBar():\n",
    "        results=delayedObj.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 py3_parcels",
   "language": "python",
   "name": "py3_parcels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
