{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0837a6c6",
   "metadata": {},
   "source": [
    "#### Running the Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002de495",
   "metadata": {},
   "source": [
    "The main simulation code can be found in the file **gom_b_sim_v2.py**.\n",
    "\n",
    "The code is split into four parts: the FieldSet, the ParticleSet, the Kernels, and the simulation run. Each of these components follow the [OceanParcels](https://docs.oceanparcels.org/en/latest/examples/tutorial_parcels_structure.html) documentation very closely. For more information on how each of these components was generated, I would recommend reading the documentation.\n",
    "\n",
    "To run the simulation, I have split up the computation into 5 runs with 6 months beaching periods between 07/01/2019 and 12/01/2021. This required creating the following SLURM scripts,\n",
    "\n",
    " - gombsim_2019_12_7.sh\n",
    " - gombsim_2020_6_1.sh\n",
    " - gombsim_2020_12_7.sh\n",
    " - gombsim_2021_6_1.sh\n",
    " - gombsim_2021_12_7.sh\n",
    "\n",
    "for the labeled beaching period. Each run simulates particles for 180 days with a release window of particles backwards in time equal to the provided window. For example a call to the simulation inside the SLURM script might look like,\n",
    "\n",
    "```python\n",
    "python gom_b_sim_v2.py data/output_v2/raw1/ 2019 12 7 180\n",
    "```\n",
    "where the first argument indicates the path to write the trajectory output, the second argument indicates the year, the third argument indicates the starting month (backwards in time), the fourth argument indicate the ending month, and the last argument indicates the number of days each particle is tracked. Note that the simulation code can only run particles within a single year period for any number of months. If we want to simulate for more than a year, the simulation code must be altered.\n",
    "\n",
    "The main idea behind setting up the simulation this way is because in OceanParcels (as of October 2023) has a fairly rudimentary parallelization module. The main issue behind its current implmentation is that it tries to optimize the amount of fieldset needed for the simulation, so it automatically chunks the simulation up based on only the particleset so it knows where in the fieldset it needs to advect particles. They have already pointed out the main issue with this method and will be fixing it. However, this will not work for us, because our particle set is not constant for each release window. For example, one day we might release 10 particles but the next day 10000 particles may be released. In the current parallelization code, this would create two chunks of 10 and 10000 particles. So one file is still running 10000 particles while the other is only running 10, which beats the purpose of doing this in the first place. To get around this, I chunked my runs ahead of time and ran everything all at once within each predetermined chunk/run. \n",
    "\n",
    "Once each run has completed, we rechunk the files based on the recommendation made here, [Dealing with large output files](https://docs.oceanparcels.org/en/latest/examples/documentation_LargeRunsOutput.html). This greatly improves reading preformance of the trajectory files. Now that we have the output of each of the 5 runs, we just loop over each file seperately, and do the relevant post-processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dd77b",
   "metadata": {},
   "source": [
    "#### Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad02ec4",
   "metadata": {},
   "source": [
    "The post processing is split up into ordered notebooks, starting with 01 and ending at 05, which contain the implmention from defining the domian to generating the final graphs. In each notebook, we also extract the revelant statistics, figures, and numbers needed in the thesis. Below, we have checked that the number of particles released with the nurdle statistic table generated in **05_plot_graphs.py**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7808c7e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f261f",
   "metadata": {},
   "source": [
    "##### Number of Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cabf3b",
   "metadata": {},
   "source": [
    "To gain information about the number trajectories we have extracted the exact number of particles from the simulation files in two ways. The first is by counting the number of non-zero trajectories directly from the output of the simulation, and the second is manually checking the SLURM output files and recording the number of particles released that was printed at the beginning of the simulation run output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8492038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Nurdle Count: 349177\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from os import path\n",
    "import xarray as xr\n",
    "\n",
    "inputDir_Sim = ('data/output_v2/rechunked/')\n",
    "inputFiles_Sim = sorted(glob(inputDir_Sim + '*.zarr' ))\n",
    "\n",
    "trajectory_counts = []\n",
    "\n",
    "# Trajectories are stored in 5 seperate files \n",
    "for filename in inputFiles_Sim:    \n",
    "    df = xr.open_zarr(filename).dropna(dim=\"trajectory\", how=\"all\")\n",
    "    trajectory_counts.append(df.dims['trajectory'])\n",
    "    \n",
    "first_count = np.sum(trajectory_counts)\n",
    "second_count = (23475+116025+103173+44915+61589)\n",
    "assert(first_count == second_count)\n",
    "print('Total Nurdle Count:', first_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0072aa1",
   "metadata": {},
   "source": [
    "##### Nurdle Statstics Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d52de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>survey_count</th>\n",
       "      <th>survey_perc</th>\n",
       "      <th>total_nurdle_count</th>\n",
       "      <th>total_nurdle_perc</th>\n",
       "      <th>mean_nurdle_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2.093</td>\n",
       "      <td>317.0</td>\n",
       "      <td>0.091</td>\n",
       "      <td>7.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Florida</td>\n",
       "      <td>66.0</td>\n",
       "      <td>3.140</td>\n",
       "      <td>2051.0</td>\n",
       "      <td>0.587</td>\n",
       "      <td>31.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>104.0</td>\n",
       "      <td>4.948</td>\n",
       "      <td>115107.0</td>\n",
       "      <td>32.965</td>\n",
       "      <td>1106.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>177.0</td>\n",
       "      <td>8.421</td>\n",
       "      <td>30725.0</td>\n",
       "      <td>8.799</td>\n",
       "      <td>173.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Texas</td>\n",
       "      <td>1711.0</td>\n",
       "      <td>81.399</td>\n",
       "      <td>200977.0</td>\n",
       "      <td>57.557</td>\n",
       "      <td>117.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Total</td>\n",
       "      <td>2102.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>349177.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1436.128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        region  survey_count  survey_perc  total_nurdle_count  \\\n",
       "0      Alabama          44.0        2.093               317.0   \n",
       "1      Florida          66.0        3.140              2051.0   \n",
       "2    Louisiana         104.0        4.948            115107.0   \n",
       "3  Mississippi         177.0        8.421             30725.0   \n",
       "4        Texas        1711.0       81.399            200977.0   \n",
       "5        Total        2102.0      100.000            349177.0   \n",
       "\n",
       "   total_nurdle_perc  mean_nurdle_count  \n",
       "0              0.091              7.205  \n",
       "1              0.587             31.076  \n",
       "2             32.965           1106.798  \n",
       "3              8.799            173.588  \n",
       "4             57.557            117.462  \n",
       "5            100.000           1436.128  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputDir = 'data/posterior_computation_data/'\n",
    "outputgraphsDir = outputDir + 'graphs/'\n",
    "\n",
    "pd.read_csv(outputgraphsDir + 'nurdle_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482de51f",
   "metadata": {},
   "source": [
    "They match!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 py3_parcels",
   "language": "python",
   "name": "py3_parcels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
